<h1 id="video-object-detection">Video Object Detection</h1>
<blockquote>
<p>All I know about video det&amp;track.<br>These two topics are <strong>NOT</strong> identical.<br>Feature extraction based ğŸ†š Metrics learning based</p>
</blockquote>
<h2 id="trendings">Trendings</h2>
<p><u><strong>LSTM</strong></u><br>mostly used in video <strong>understanding</strong>, eg: video abnormal detection, event recognization, find contentâ€¦<br>Extract global action &amp; scene information</p>
<p><u><strong>Detect+Track</strong></u></p>
<blockquote>
<p>How to leverage temporal information?</p>
</blockquote>
<p>Tracking: ææ¨¡ç‰ˆç‰¹å¾ï¼Œç‰¹å¾å›¾åŒ¹é…ï¼Œæ‰¾<br>Detection in video: </p>
<ol>
<li><p>frame by frame</p>
</li>
<li><p>ä½¿ç”¨temporal informationä½œä¸º<u><strong>ç±»åˆ«</strong></u>åˆ¤æ–­çš„ä¾æ®<br>ä½¿ç”¨LSTMä¼ é€’æ—¶é—´ä¿¡æ¯ï¼ˆ<strong>any context information?</strong>ï¼‰</p>
</li>
<li><p>ä½¿ç”¨temporalé¢„æµ‹å¯èƒ½å‡ºç°çš„ä½ç½®ï¼Œ<u><strong>ä¸ç¡®å®šæ€§</strong></u><br>Fuse æ£€æµ‹ä½ç½®+é¢„æµ‹ä½ç½® with uncertainty<br>Multi hypothesis tracking</p>
</li>
</ol>
<p><u>Detect to Track and Track to Detect Papers:</u></p>
<p><em>Detect to Track and Track to Detect</em> <a href="https://github.com/feichtenhofer/Detect-Track">https://github.com/feichtenhofer/Detect-Track</a><br><em>Integrated Object Detection and Tracking with Tracklet-Conditioned Detection</em></p>
<p><u><strong>video object segmentation</strong></u> hot topic.    <strong>datasets</strong>: youtube-VOS, DAVIS</p>
<ol>
<li><em>Spatiotemporal CNN for Video Object Segmentation</em> use <strong>LSTM</strong>, two branch, <strong>attention mechanism</strong></li>
<li><em>See More, Know More: Unsupervised Video Object Segmentation With Co-Attention Siamese Networks</em>  apply <strong>co-attention</strong></li>
<li><em>Fast User-Guided Video Object Segmentation by Interaction-And-Propagation Networks</em></li>
<li><em>RVOS: End-To-End Recurrent Network for Video Object Segmentation</em></li>
<li><em>BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames</em>        does not have to be <strong>first</strong> frame, and <strong>select</strong> the best frame in the training sets, <strong>ranking frame</strong> mechanism</li>
<li><em>FEELVOS: Fast End-To-End Embedding Learning for Video Object Segmentation</em>    use <strong>pixel-wise embedding</strong> and global&amp;local <strong>matching</strong> mechanism to <strong>transfer</strong> the information from first and previous to current frame</li>
<li><em>Object Discovery in Videos as Foreground Motion Clustering</em>    model the VOS problem as foreground motion clustering, <strong>cluster foreground pixel into different object</strong>. Use RNN to learn embedding of fore-pixel trajectory, add correspondence of pixels in frames.</li>
<li>MHP-VOS: Multiple Hypotheses Propagation for Video Object Segmentation_    <strong>multiple hypothesis tracking</strong></li>
</ol>
<p><u><strong>Re-ID in video</strong></u><br><em>Attribute-Driven Feature Disentangling and Temporal Aggregation for Video Person Re-Identification</em>        <strong>attribute</strong>-driven feature disentangling &amp; frame <strong>re-weighting</strong><br><em>VRSTC: Occlusion-Free Video Person Re-Identification</em>    use temporal information to <strong>recover</strong> occluded frame</p>
<p><u><strong>fusion</strong></u> spatial and temporal feature, using <strong>weighted sum</strong>, optical flow<br><em>Accel: A Corrective Fusion Network for Efficient Semantic Segmentation on Video</em></p>
<p><u><strong>unsupervised manner</strong></u> add other training signal</p>
<p><u><strong>weakly-supervised manner</strong></u> use motion and video clue to generate more precise <strong>proposals</strong>.<br><em>You Reap What You Sow: Using Videos to Generate High Precision Object Proposals for Weakly-Supervised Object Detection</em> </p>
<p><u><strong>graph convolution network</strong></u> perform temporal reasoning</p>
<p><u><strong>downsampling</strong></u> is sometimes beneficial in terms of accuracy. By means of 1) reducing unnecessary details    2) resize the too-large objects and increase confidence <em>Adascale: Towards Real-time video object detection using adaptive scaling</em> </p>
<p><u><strong>utilize temporal information</strong></u> 1. <strong>wrap</strong> temporal info with feature to generate future feature     2. for partial <strong>occlusion</strong>, motion <strong>blur</strong> in video</p>
<p><u><strong>iteratively refine</strong></u><br><em>STEP: Spatio-Temporal Progressive Learning for Video Action Detection</em><br>refine the proposal to action, step by step. Spatial-temporal: spatial displacement + action tube(temporal info)</p>
<h2 id="datasets">Datasets</h2>
<ol>
<li>ImageNet VID: <a href="http://image-net.org/challenges/LSVRC/2017/#vid">ILSVRC2017</a><br>30 categories<br>2015:<br><em>train</em>     1952 snippets, 405014 (186358+218656) images<br><em>test</em>     458 snippets, 127618 images<br><em>val</em>     281 snippets, 64698 images<br>2017:<br><em>train</em>     3862 snippets, 1122397 images<br><em>test</em>     937 snippets, 315176 images<br><em>val</em>     555 snippets, 176126 images</li>
<li>Youtube-BB<br>5.6M bounding boxes<br>240k snippets    (380k in paper, about 19s long)<br>23 categories, <em>NONE</em> category for unseen category<br>Annotate video with 1 frame per second</li>
<li>UA-DETRAC</li>
<li>UAVDT</li>
<li>MOT challenge (Design for MOT)</li>
</ol>
<hr>
<h2 id="sotas">SOTAs</h2>
<p><em>Integrated Object Detection and Tracking with Tracklet-Conditioned Detection</em><br> <a href="https://paperswithcode.com/paper/integrated-object-detection-and-tracking-with">Tracklet-Conditioned Detection+DCNv2+FGFA</a><br>mAP=83.5 </p>
<p>Integrate tracking in detection not post processing<br>Compute <strong>embeddings</strong> of tracking trajectory with detection box, embeddings-weighted <strong>sum</strong> trajectory category <strong>confidence</strong> with detect category confidence. </p>
<p>Weight = f(embeddings)<br>Update trajectory confidence with new + old<br>Class confidence = trajectory confidence + det confidence<br>Output = weighted-sum(weights*Class confidence)</p>
<p><strong>Category(only)</strong> is determined jointly weighted by last trajectory category and detect box category</p>
<p><u><strong>code</strong></u> released <a href="https://paperswithcode.com/paper/flow-guided-feature-aggregation-for-video">Flow-Guided Feature Aggregation for Video Object Detection</a><br>mAP=80.1, 2017<br>code released</p>
<h2 id="thinkings">Thinkings</h2>
<ol>
<li><u><strong>No keyframe</strong></u>  use LSTM to <strong>directly</strong> generate detection result<br>Input image -&gt; <strong>every frame</strong>, LSTM to hidden layer and output bbox.</li>
<li><u><strong>Keyframe</strong></u>  select only keyframe for deep and warp to generate interval frameâ€™s feature map (based on <strong>optical flow</strong>)<br>ğŸ‘† How to get feature map with low cost</li>
</ol>
<hr>
<p>ğŸ‘‡ How to get box with previous information </p>
<ol start="3">
<li><u><strong>Tracking based</strong><u>  detect by tracking and tracking by detect</li>
</ol>
<h4 id="detection-and-tracking">Detection and Tracking</h4>
<p>åšvideo detection <img src="https://latex.codecogs.com/svg.latex?\to" alt="latex_equ"> é¿å¼€trackingï¼šç‰©ä½“ä¸åŠ¨ï¼Œåˆ†ç±»ï¼Œ3Dæ¡†ï¼Œä½¿ç”¨LSTMç‰¹å¾ä¼ æ’­ï¼ˆä¸€å¸§æ•ˆæœå·®ï¼Œå¤šå¸§åºåˆ—å˜å¥½ï¼‰<br>é™æ€å›¾ç‰‡detection</p>
<blockquote>
<p>Why temporal information is not leveraged in tracking?  </p>
</blockquote>
<p>éš¾ç‚¹ï¼šå¸§é—´ä¿¡æ¯ï¼Œtemporalä¿¡æ¯çš„é«˜æ•ˆä¼ é€’<br>ä¼ é€’æ¸…æ™°ä¿¡æ¯ï¼Œé˜²æ­¢<u><strong>motion blur</strong></u><br><u><strong>tubelet</strong></u></p>
<h2 id="topics-in-workshop">Topics in Workshop</h2>
<p>-- <strong>Large scale</strong> surveillance video: <a href="http://gigavision.cn/">GigaVision</a></p>
<p>â€” Autonomous driving: <a href="http://wad.ai/2019/challenge.html">Workshop on autonomous driving</a> <u><strong>3D bounding box</strong></u> Baidu Apollos</p>
<p>â€” <strong>Aerial</strong> image (remote sensor): <a href="https://captain-whu.github.io/DOAI2019/cfp.html">Detecting Objects in Aerial Images (DOAI)</a><br>éš¾ç‚¹ï¼š1. Scale variance    2. Small object densely distributed        3. Arbitrary orientation </p>
<p>â€” <strong>UAV</strong>ision:  <a href="https://sites.google.com/site/uavision2019/home">https://sites.google.com/site/uavision2019/home</a> <u><strong>UAV</strong></u>     1920x1080, 15m, 2min, <em>no classification</em></p>
<p>â€” <strong>MOT</strong>: <a href="https://motchallenge.net/workshops/bmtt2019/index.html">BMTT MOTChallenge 2019</a></p>
<p>â€” <strong>ReId</strong>, Multi-target multi-camera tracking: <a href="https://reid-mct.github.io/2019/">Target Re-identification and Multi-Target Multi-Camera Tracking</a></p>
<p>â€” <strong>Autonomous driving</strong>: <a href="https://sites.google.com/view/wad2019/challenge">https://sites.google.com/view/wad2019/challenge</a><br>D2-city: 10k video, 1k for tracking, HD<br>BDD100k: 100k video, nano on keyframe, 40s, 720p 30fps <a href="https://interestingengineering.com/you-can-now-download-the-worlds-largest-self-driving-dataset">You Can Now Download the Worldâ€™s Largest Self-Driving Dataset</a><br>nuScenes: 1.4M frames, <u><strong>3D box annotation</strong></u><br>Other autonomous driving datasets: Oxford Robotcar, TorontoCity, KITTI, <em>Apollo Scape</em> (1M), <em>Waymo Open Dataset</em> (16.7h, 600k frame, 22m 2D-bbox)  <a href="https://scale.com/open-datasets">https://scale.com/open-datasets</a> </p>
<h2 id="papers-at-eccv18">Papers at ECCV18</h2>
<p><strong>Temporal information for Classifying</strong> <em>Multi-Fiber Networks for Video Recognization (ECCV18)</em><br><strong>All</strong> <em>Fully Motion-Aware Network for Video Object Detection</em><br><em>Video Object Detection with an Aligned Spatial-Temporal Memory</em><br><strong>Hard example mining</strong> <em>Unsupervised Hard Example Mining from Videos for Improved Object Detection</em><br><strong>Sampling?</strong> <em>Object Detection in Video with Spatiotemporal Sampling Networks</em><br><em>3D Tracking &amp; Trajectory</em> <em>3D Vehicle Trajectory Reconstruction in Monocular Video Data Using Environment Structure Constraints</em></p>
<blockquote>
<p>RCNN -&gt; Fast RCNN: ä½¿ç”¨RoI poolingä»£æ›¿resizeï¼Œåªè®¡ç®—ä¸€æ¬¡ç‰¹å¾å›¾(RoI projection)ï¼Œå¤šä»»åŠ¡è®­ç»ƒ(bbox regre.å’Œclassif.ä¸€èµ·è®­ç»ƒ)<br>Fast RCNN -&gt; Faster RCNN: ä½¿ç”¨RPNä»£æ›¿selective search  </p>
</blockquote>
<p><img src="Figures/6F235DE5-4774-4527-9FC9-F8EAF8252AEE.png" alt=""><br>ä¸€é˜¶æ®µç›¸æ¯”äºŒé˜¶æ®µå°‘äº†RoI poolingè¿‡ç¨‹ï¼Œæ‹¿åˆ°æ¡†ç›´æ¥åœ¨æ•´å¼ å›¾çš„ç‰¹å¾å›¾ä¸Šåˆ†ç±»å›å½’ï¼Œè€Œä¸åœ¨æ¡†ä¸­è¿›è¡Œã€‚å¯¼è‡´å¯èƒ½ç‰¹å¾åç§»é—®é¢˜</p>
<hr>
<h1 id="papers">Papers</h1>
<h2 id="object-detection-in-video-with-saptiotemporal-sampling-networks">Object Detection in Video with Saptiotemporal Sampling Networks</h2>
<blockquote>
<p>ä½¿ç”¨ç±»ä¼¼FGFAçš„æ–¹æ³•ï¼Œä½†æ˜¯å¢åŠ deformableå·ç§¯ï¼Œç®€åŒ–æ±‚å…¶ä»–å¸§featureå’Œæƒé‡çš„æ­¥éª¤</p>
</blockquote>
<h4 id="motivation">Motivation</h4>
<p>å»æ‰è®­ç»ƒä¸­éœ€è¦çš„å…‰æµæ•°æ®ï¼Œæå‡ï¼ˆè®­ç»ƒï¼‰é€Ÿåº¦</p>
<h4 id="approach">Approach</h4>
<p><strong>Deformable Convolution</strong>: é€šè¿‡æ•°æ®è®¡ç®—å‡ºçš„åç§»é‡ï¼Œæ˜¯å·ç§¯çš„receptive fieldå¯å˜ã€‚ä¸åªæ˜¯åŸºäºä¸­å¿ƒçš„<code>{(-1,-1),(-1,0),(-1,1),...,(1,0),(1,1)}</code>ï¼Œå³<img src="https://latex.codecogs.com/svg.latex?p_0+p_n" alt="latex_equ">ï¼Œè€Œå¯ä»¥æ˜¯<img src="https://latex.codecogs.com/svg.latex?p_0+p_n+\Delta%20p_n" alt="latex_equ">ã€‚å…¶ä¸­<img src="https://latex.codecogs.com/svg.latex?\Delta%20p_n" alt="latex_equ">ä¸ºå°æ•°ï¼Œä½¿ç”¨åŒçº¿æ€§æ’å€¼è®¡ç®—ã€‚</p>
<p><strong>Spatiotemporal Sampling Network</strong></p>
<p>é€‰æ‹©å‰åKå¸§çš„ç‰¹å¾å›¾è¿›è¡Œèåˆï¼Œå½“å‰å¸§reference frameï¼Œå…¶ä»–å¸§supporting frameã€‚</p>
<ol>
<li>æ±‚ç‰¹å¾æ—¶è¿›è¡Œå››æ¬¡<strong>å˜å½¢å·ç§¯</strong></li>
</ol>
<p><img src="https://latex.codecogs.com/svg.latex?f_t=Backbone%28I_t%29,\;%20f_{t+k}=Backbone%28I_{t+k}%29,\;%20f_{t,t+k}=concat%28f_t,%20f_{t+k}%29%20" alt="latex_equ"></p>
<p><img src="https://latex.codecogs.com/svg.latex?o^{%281%29}_{t,t+k}=predict\_offset%28f_{t,t+k}%29" alt="latex_equ"></p>
<p><img src="https://latex.codecogs.com/svg.latex?g^{%281%29}_{t,t+k}=deform\_conv%28f_{t,t+k},\;%20o^{%281%29}_{t,t+k}%29%20" alt="latex_equ"></p>
<p><img src="https://latex.codecogs.com/svg.latex?o^{%282%29}_{t,t+k}=predict\_offset%28g^{%281%29}_{t,t+k}%29%20" alt="latex_equ"></p>
<p><img src="https://latex.codecogs.com/svg.latex?g^{%282%29}_{t,t+k}=deform\_conv%28g^{%281%29}_{t,t+k},\;%20o^{%282%29}_{t,t+k}%29" alt="latex_equ"></p>
<p>And so on...</p>
<p>ä½†æœ€åä¸€æ¬¡ï¼Œä½¿ç”¨æœ€åˆçš„</p>
<p><img src="https://latex.codecogs.com/svg.latex?g^{%284%29}_{t,t+k}=deform\_conv%28o^{%284%29}_{t,t+k},\;%20f_{t,t+k}%29" alt="latex_equ"></p>
<ol start="2">
<li>èåˆæ—¶ï¼Œå°†å‰åKå¸§è¿›è¡Œèåˆã€‚</li>
</ol>
<p>è®¡ç®—ç¬¬<code>t+k</code>å¸§æƒé‡ï¼š</p>
<p>ä¸‰å±‚å­ç½‘ç»œSå¯¹gè®¡ç®—ä¸­é—´è¡¨ç¤ºï¼Œæ±‚ä½™å¼¦è·ç¦»çš„expæ¥è®¡ç®—æƒå€¼ã€‚å¯¹å‰åçš„æ¯ä¸€å¼ support frameçš„æ¯ä¸€ä¸ªåƒç´ pè®¡ç®—èåˆæƒé‡</p>
<p><img src="https://latex.codecogs.com/svg.latex?w_{t,t+k}%28p%29=exp%28\frac{S%28g^{%284%29}_{t,t}%29%28p%29\cdot%20S%28g^{%284%29}_{t,t+k}%29%28p%29}{|S%28g^{%284%29}_{t,t}%29%28p%29|\;|S%28g^{%284%29}_{t,t+k}%29%28p%29|}%29" alt="latex_equ"></p>
<p>å½’ä¸€åŒ–åèåˆï¼Œåœ¨t-Kåˆ°t+Kçš„æ—¶é—´èŒƒå›´ä¸ŠåŠ æƒæ±‚å’Œï¼Œè·å¾—æ¯ä¸ªåƒç´ ç‚¹åœ¨reference frameï¼ˆtæ—¶åˆ»ï¼‰çš„èåˆç‰¹å¾ï¼Œè¾“å…¥æ£€æµ‹ç½‘ç»œã€‚</p>
<p><strong>ç»†èŠ‚</strong></p>
<ul>
<li><p>backboneé‡‡ç”¨å¢åŠ 4ä¸ª<img src="https://latex.codecogs.com/svg.latex?3\times%203" alt="latex_equ">å˜å½¢å·ç§¯çš„ResNet-101ç½‘ç»œã€‚</p>
</li>
<li><p>è·å¾—èåˆç‰¹å¾<img src="https://latex.codecogs.com/svg.latex?g^{aggr.}_t" alt="latex_equ">åï¼Œæ‹†æˆä¸¤éƒ¨åˆ†ï¼Œä¸€åŠè¾“å…¥RPNäº§ç”Ÿproposalï¼ˆæ¯ç‚¹9ä¸ªanchorå’Œä¸€å…±300ä¸ªproposalï¼‰ï¼Œå¦ä¸€åŠè¾“å…¥R-FCNã€‚</p>
</li>
<li><p>è®­ç»ƒæ—¶Kè¾ƒå°ï¼ŒK=1ï¼Œå‰åå„ä¸€å¸§ï¼Œéšæœºsampleçš„ã€‚</p>
</li>
<li><p>å…ˆåœ¨DETä¸Šé¢„è®­ç»ƒï¼Œsupport frameå°±æ˜¯æœ¬èº«ã€‚</p>
</li>
<li><p>æµ‹è¯•æ—¶ä½¿ç”¨è¾ƒå¤§Kï¼ŒK=13ã€‚å…ˆç®—å‡ºç‰¹å¾å›¾ç„¶åç¼“å­˜æ¥è§£å†³GPU RAMé—®é¢˜ã€‚</p>
</li>
</ul>
<hr>
<h2 id="looking-fast-and-slow-memory-guided-mobile-video-object-detection">Looking Fast and Slow: Memory-Guided Mobile Video Object Detection</h2>
<blockquote>
<p>Using memory(LSTM) in object detection</p>
<p>SOTA of ImageNet VID</p>
</blockquote>
<p>Concern more on light-weight and low computation time.</p>
<p>ä½¿ç”¨è½»é‡çº§ç½‘ç»œmobilenetè¯†åˆ«åœºæ™¯çš„ä¸»è¦å†…å®¹ï¼Œå¿«é€Ÿçš„ç‰¹å¾æå–éœ€è¦ç»´æŠ¤memoryä½œä¸ºè¡¥å……ä¿¡æ¯</p>
<p>ä¸€ä¸ªç²¾ç¡®çš„ç‰¹å¾æå–å™¨ç”¨äºåˆå§‹åŒ–å’Œç»´æŠ¤memoryï¼Œä¹‹åå¿«é€Ÿå¤„ç†ï¼Œä½¿ç”¨LSTMç»´æŠ¤memoryã€‚å¼ºåŒ–å­¦ä¹ ç”¨æ¥å†³å®šä½¿ç”¨å¿«é€Ÿ/æ…¢é€Ÿç‰¹å¾æå–å™¨(tradeoff)</p>
<hr>
<h4 id="-">å¤šåˆ†æ”¯ç‰¹å¾æå–</h4>
<p>Use two feature extractor <strong>parallel</strong> (accuracyğŸ†šspeed)</p>
<p><img src="Figures/image-20191017232410901.png" alt="image-20191017232410901"></p>
<p>inferenceæµç¨‹</p>
<p><img src="https://latex.codecogs.com/svg.latex?M_k,%20s_k%20=%20\bold{m}%28\bold{f_i}%28I_k%29,%20s_{k-1}%29" alt="latex_equ"></p>
<p><img src="https://latex.codecogs.com/svg.latex?D_k=\bold{d}%28M_k%29" alt="latex_equ"></p>
<p><img src="https://latex.codecogs.com/svg.latex?\bold{f_i}" alt="latex_equ">ä¸ºé€‰æ‹©çš„ç‰¹å¾æå–ç½‘ç»œï¼Œ<strong>m</strong>ä¸ºmemory module.</p>
<p><img src="https://latex.codecogs.com/svg.latex?\bold{f_i}=\{f_0:%20MobileNetV2%20\to%20accuracy,\;\;%20f_1:%20low\;%20reso%20\&amp;%20depth%20\to%20speed\}" alt="latex_equ">ï¼Œ<strong>d</strong>ä¸ºSSDæ£€æµ‹ç½‘ç»œ</p>
<p>å®šä¹‰<img src="https://latex.codecogs.com/svg.latex?\tau" alt="latex_equ">ä¸º<img src="https://latex.codecogs.com/svg.latex?f_1:f_0" alt="latex_equ">è¶…å‚æ•°ï¼Œä¹Ÿå¯ä»¥é€šè¿‡interleaving policyè·å¾—</p>
<p><strong>other methods</strong>ï¼šå‡å°‘æ·±åº¦0.35ï¼Œé™ä½åˆ†è¾¨ç‡160x160ï¼ŒSSDLiteï¼Œé™åˆ¶anchorçš„é•¿å®½æ¯”<img src="https://latex.codecogs.com/svg.latex?\{1,\;0.5,\;2.0\}" alt="latex_equ"></p>
<h4 id="memory-module">memory module</h4>
<p><img src="Figures/image-20191018103006549.png" alt="image-20191018103006549"></p>
<p>Modified LSTM moduleğŸ‘†: </p>
<ol>
<li><strong>skip connection</strong> between the bottleneck and output</li>
<li><strong>grouped convolution</strong> process LSTM state groups separately</li>
</ol>
<p><em>Ps. standard LSTM</em>ğŸ‘‡</p>
<p><img src="Figures/image-20191018103139589.png" alt="image-20191018103139589"></p>
<p>To perserve <strong>long-term dependencies</strong> <img src="https://latex.codecogs.com/svg.latex?\to" alt="latex_equ"> <em>skip state update</em>: when <img src="https://latex.codecogs.com/svg.latex?f_1" alt="latex_equ"> run, always <strong>reuse output state</strong> from the last time <img src="https://latex.codecogs.com/svg.latex?f_0" alt="latex_equ"> was run</p>
<h4 id="training">Training</h4>
<p>Pretrain LSTM on Imagenet Cls for initialization</p>
<p>Unroll LSTM to six steps</p>
<p>Random select feature extractor</p>
<p>Crop and shift to augment training data</p>
<hr>
<h4 id="adaptive-interleaving-policy-rl-">Adaptive Interleaving Policy(RL)</h4>
<p>Policy network <img src="https://latex.codecogs.com/svg.latex?\pi" alt="latex_equ"> to measure detection confidence, examines <strong>LSTM state</strong> and decide next feature extractor to run</p>
<p>Train policy network using Double Q-learning(DDQN)</p>
<p>Action space: <img src="https://latex.codecogs.com/svg.latex?f_i" alt="latex_equ"> at next step</p>
<p>State space: <img src="https://latex.codecogs.com/svg.latex?S=%28c_t,\;h_t,\;c_t-c_{t-1},\;h_t-h_{t-1},\;\eta_t%29" alt="latex_equ">, LSTM states and their changes, action history term <img src="https://latex.codecogs.com/svg.latex?\eta" alt="latex_equ"> (binary vector, len=20).</p>
<p>Reward space: <strong>speed reward</strong> positive reward when <img src="https://latex.codecogs.com/svg.latex?f_1" alt="latex_equ"> is run, <strong>accuracy reward</strong> loss difference between min-loss extractor.</p>
<p><img src="Figures/image-20191018120813917.png" alt="image-20191018120813917"></p>
<p>Policy network to devide which extractorğŸ‘‡</p>
<p><img src="Figures/image-20191018121558765.png" alt="image-20191018121558765"></p>
<p>Generate batches of <img src="https://latex.codecogs.com/svg.latex?%28S_t,\;a,\;S_{t+1},\;R_t%29" alt="latex_equ"> by run interleaved network in inference mode</p>
<p>Training processğŸ‘‡</p>
<p><img src="Figures/image-20191018121739427.png" alt="image-20191018121739427"></p>
<hr>
<h4 id="inference-optimization">Inference Optimization</h4>
<ol>
<li>Asynchronous mode</li>
</ol>
<p><img src="https://latex.codecogs.com/svg.latex?f_0" alt="latex_equ"> and <img src="https://latex.codecogs.com/svg.latex?f_1" alt="latex_equ"> run in separate threads,  <img src="https://latex.codecogs.com/svg.latex?f_1" alt="latex_equ"> keeps detection and  <img src="https://latex.codecogs.com/svg.latex?f_0" alt="latex_equ"> <strong>updates memory when finished</strong> its computation. Memory module use most recent available memory, <strong>NO WAIT</strong> for slow extractor.</p>
<p><strong>Potential Weakness</strong>: latency/mismatch of call large extractor and accuracy memory output. Delay of generate more powerful memory using large extractor when encounter hard example. Memory will remains less powerful before large extractor generates new one.</p>
<ol start="2">
<li>Quantization</li>
</ol>
<hr>
<h4 id="experiments">Experiments</h4>
<p><img src="Figures/image-20191018141931896.png" alt="ImageNetVID-val"></p>
<p>ImageNet VID valğŸ‘†</p>
<p><img src="Figures/image-20191018142044512.png" alt="image-20191018142044512"></p>
<p>ğŸ‘†RL demonstration: red means call large model, blue for small model.</p>
<hr>
<h2 id="object-detection-in-videos-with-tubelet-proposal-networks">Object detection in videos with Tubelet Proposal Networks</h2>
<blockquote>
<p>å¦‚ä½•é«˜æ•ˆçš„äº§ç”Ÿæ—¶é—´ç»´åº¦çš„proposal (aka. ::tubelet::)?<br>é€šè¿‡å…³é”®å¸§æ£€æµ‹ç»“æœäº§ç”Ÿä¸€æ¡åºåˆ—çš„æ‰€æœ‰proposal ::detect by track::ã€‚ç„¶åä½¿ç”¨LSTMåˆ†ç±»  </p>
</blockquote>
<p>äº§ç”Ÿtubeletæœ‰ä¸¤ç§æ–¹æ³• 1. Motion-based (only for short-term)    2. Appearance-based (tracking, expensive/?)</p>
<h4 id="approach">Approach</h4>
<p><img src="Figures/8894F10F-8B2A-407E-84A0-1704CEB15B71.png" alt=""><br>â†–ï¸é¦–å…ˆå¯¹é™æ€å›¾ç‰‡è¿›è¡Œæ£€æµ‹è·å¾—æ£€æµ‹ç»“æœï¼Œç„¶ååœ¨ <strong>ç›¸åŒä½ç½®</strong> ä¸åŒæ—¶é—´ä¸Špoolingï¼Œè·å¾—spatial anchorsã€‚åŸºäºå‡è®¾æ„Ÿå—é‡è¶³å¤Ÿå¤§å¯ä»¥è·å¾—è¿åŠ¨ç‰©ä½“çš„ç‰¹å¾ï¼ˆä¸­å¿ƒä¸ä¼šç§»å‡ºç‰©ä½“æ¡†ï¼‰ã€‚Alignä¹‹åç”¨äºé¢„æµ‹ç‰©ä½“çš„ç§»åŠ¨</p>
<p>ä½¿ç”¨Tubelet Proposal Networkå›å½’ç½‘ç»œé¢„æµ‹ç›¸å¯¹äº <strong>ç¬¬ä¸€å¸§</strong> çš„è¿åŠ¨é‡ï¼ˆä¸ºäº†é˜²æ­¢è¿½è¸ªè¿‡ç¨‹ä¸­çš„driftï¼Œç´¯è®¡è¯¯å·®ï¼‰ã€‚é¢„æµ‹çš„æ—¶é—´åºåˆ—é•¿åº¦ä¸ºomega<br><img src="Figures/3C21D393-7BED-48D6-B7E6-91B8706DE95E.png" alt=""><br><img src="Figures/056FAC3D-B7C8-491D-855C-D17FC615DFB2.png" alt=""><br><img src="Figures/CDE6301A-A0BC-47A8-AFBA-17C416E72EE4.png" alt=""><br>åŒæ—¶ï¼Œè®¤ä¸ºGTçš„bboxå°±æ˜¯tubelet proposalçš„ç›‘ç£ä¿¡å·ã€‚åŒæ—¶å¯¹è¿åŠ¨è¡¨ç¤ºè¿›è¡Œå½’ä¸€åŒ–ã€‚ï¼ˆå¯¹å½’ä¸€åŒ–åçš„æ®‹é‡è¿›è¡Œå­¦ä¹ ï¼‰<br><img src="Figures/A251E513-8863-44A3-966C-BB760D6F5621.png" alt=""><br>æŸå¤±å‡½æ•°ğŸ‘‡<br><img src="Figures/34F1D7B2-078A-4DFE-AB19-6CDF87EB0305.png" alt=""><br>ğŸ‘†Mä¸ºGTï¼ŒM_hatä¸ºå½’ä¸€åŒ–åçš„offset</p>
<p>åˆ›æ–°ç‚¹ï¼š::åˆ†å—åˆå§‹åŒ–::<br>é¦–å…ˆè®­ç»ƒé¢„æµ‹æ—¶é—´åºåˆ—é•¿åº¦ä¸º2çš„TPNï¼Œå¾—åˆ°å‚æ•°W_2å’Œb_2ã€‚ç”±äºç¬¬äºŒå¸§è¿åŠ¨é‡m_2ç”±ç¬¬1å’Œç¬¬2å¸§çš„ç‰¹å¾å›¾é¢„æµ‹ï¼Œç¬¬ä¸‰å¸§è¿åŠ¨é‡ç”±ç¬¬1å’Œç¬¬3å¸§ç‰¹å¾å›¾é¢„æµ‹ï¼Œm_4ç”±ç¬¬1&amp;4å¸§é¢„æµ‹ã€‚å’Œä¸­é—´å¸§æ— å…³ï¼Œæ‰€ä»¥è®¤ä¸ºé¢„æµ‹è¿‡ç¨‹æœ‰ç›¸ä¼¼æ€§ï¼ˆ1&amp;2 -&gt; m2, 1&amp;3 -&gt; m3)ï¼Œå¯ä»¥ä½¿ç”¨W_2å’Œb_2éƒ¨åˆ†åˆå§‹åŒ–W_3å’Œb_3å‚æ•°ä¸­çš„ä¸€å—ğŸ‘‡<br><img src="Figures/2A2C3988-1C18-4890-A0CB-BAD7549D206F.png" alt=""><br>æœ€åå¾ªç¯äº§ç”Ÿæ‰€æœ‰å¸§çš„æ‰€æœ‰static anchorçš„tubelet proposalğŸ‘‡<br><img src="Figures/D8CB4929-8B8C-4368-914C-37B6B056EC67.png" alt=""><br>LSTMåšç±»åˆ«é¢„æµ‹â†˜ï¸<br><img src="Figures/B007A05F-18CF-4E95-AD77-9E02FA3CC107.png" alt=""><br>â†—ï¸RoI-poolingä¹‹åçš„tubelet proposalä¸­ç‰¹å¾æ”¾å…¥ä¸€å±‚çš„LSTM encoderï¼Œå†å°†memoryå’Œhiddenæ”¾å…¥decoderååºè¾“å‡ºç±»åˆ«é¢„æµ‹</p>
<hr>
<h2 id="iou-tracker">IoU tracker</h2>
<p><img src="Figures/v2-049b75081ce8ddc637894d1d980c8316_hd.jpg" alt=""><br><code>D</code>è¡¨ç¤ºæ£€æµ‹ç»“æœï¼Œ<code>F</code>å¸§ï¼Œæ¯ä¸€å¸§è‡³å¤š<code>N</code>ä¸ªæ£€æµ‹ç»“æœ<br><code>T_a</code>è¡¨ç¤ºæ­£åœ¨è¿½è¸ªæœªç»“æŸçš„ç›®æ ‡ï¼Œ<code>T_f</code>è¡¨ç¤ºå·²ç»æœ€ç»ˆå®Œæˆçš„trajectoryï¼ˆç§»å‡ºç”»é¢å¤–ï¼‰<br><strong>æ€è·¯</strong>ï¼š<br>å¯¹äº <em>æŸä¸€å¸§</em> ï¼Œå¯¹äºæ¯ä¸ªæ­£åœ¨è¿½è¸ªçš„ <em>trajectory</em> ï¼Œåœ¨å½“å‰å¸§çš„æ£€æµ‹ç»“æœä¸­æ‰¾IoUæœ€å¤§çš„æ£€æµ‹ç»“æœã€‚å¦‚æœIoUå¤§äºé˜ˆå€¼ï¼Œæ·»åŠ åˆ°æ£€æµ‹ç»“æœä¸­ï¼›å¦‚æœæœ€å¤§çš„IoUéƒ½æ²¡æœ‰å¤§äºé˜ˆå€¼ï¼Œåˆ™åˆ¤æ–­trajectoryçš„é•¿åº¦å’Œæœ€é«˜ç½®ä¿¡åº¦ï¼Œåˆ¤æ–­æ˜¯å¦ä»<code>T_a</code>åˆ é™¤å¹¶åŠ å…¥æ£€æµ‹å®Œæˆtrajectoryé›†åˆä¸­<code>T_f</code>ã€‚è®¤ä¸ºæ¶ˆå¤±/è¿½è¸ªå®Œæˆ<br>ç»§ç»­ä¸‹ä¸€ä¸ªtrajectoryã€‚å‰©ä½™çš„æ£€æµ‹æ¡†ï¼Œå»ºç«‹ä¸€ä¸ªæ–°çš„trajectoryã€‚<br>æœ€å<code>T_a</code>ä¸­trajectoryåˆ¤æ–­é•¿åº¦å’Œæœ€é«˜ç½®ä¿¡åº¦ï¼Œå†³å®šæ˜¯å¦åŠ å…¥<code>T_f</code><br><code>T_f</code>å³ä¸ºè¿½è¸ªç»“æœ</p>
<hr>
<h2 id="multiple-hypothesis-tracking">Multiple Hypothesis Tracking</h2>
<h4 id="-">æ„å»ºè·Ÿè¸ªæ ‘</h4>
<p>æ¯ä¸€å¸§çš„è§‚æµ‹äº§ç”Ÿä¸€ä¸ªè·Ÿè¸ªæ ‘ï¼Œå°†å‡ºç°åœ¨geting areaçš„è§‚æµ‹æ·»åŠ ä½œä¸ºå…¶å­èŠ‚ç‚¹<br>å¢åŠ ä¸€ä¸ªåˆ†æ”¯æ ‡è®°è·Ÿè¸ªä¸¢å¤±çš„èŠ‚ç‚¹</p>
<h4 id="mahalonobis-distance">Mahalonobis Distance</h4>
<p><strong>Measure the distance between a vector(point) and a distribution</strong></p>
<blockquote>
<p>Why use Mahalonobis distance?</p>
</blockquote>
<ol>
<li>normalized:<br>normalize the distribution into <img src="https://latex.codecogs.com/svg.latex?%28x-\bar%20x%29/\sigma" alt="latex_equ"></li>
<li>consider all the sample points in the distribution, not the center of distribution only, especially when the two random variable is correlated.<br><img src="https://images-1256050009.cos.ap-beijing.myqcloud.com/15688765365463.jpg" alt="15688765365463"></li>
</ol>
<blockquote>
<p>How is Mahalonobis distance different from Euclidean distance?</p>
</blockquote>
<ol>
<li>It transforms the columns into uncorrelated variables</li>
<li>Scale the columns to make their variance equal to 1</li>
<li>Finally, it calculates the Euclidean distance.</li>
</ol>
<p><strong>formula</strong><br><img src="https://latex.codecogs.com/svg.latex?D^2=%28x-m%29^T\cdot%20C^{-1}\cdot%20%28x-m%29" alt="latex_equ"><br><img src="https://latex.codecogs.com/svg.latex?x" alt="latex_equ"> is the observation<br><img src="https://latex.codecogs.com/svg.latex?m" alt="latex_equ"> is the mean value of the independent variables<br><img src="https://latex.codecogs.com/svg.latex?C^{-1}" alt="latex_equ"> is the inverse of covariance matrix<br><a href="https://www.machinelearningplus.com/statistics/mahalanobis-distance/">Read more</a></p>
<h4 id="kalman-filter-an-estimation-method">Kalman Filter: an estimation method</h4>
<blockquote>
<p>Why use kalman filter?</p>
</blockquote>
<p>Estimate state of a system from different sources that may be subject to noise. <em>Observe external, predict internal</em><br>Fuse the observations to estimate</p>
<p><img src="https://images-1256050009.cos.ap-beijing.myqcloud.com/15688950199889.jpg" alt="15688950199889"><br><strong>formulas</strong> ps. <img src="https://latex.codecogs.com/svg.latex?\dot{x}" alt="latex_equ"> means the derivate of x<br><img src="https://latex.codecogs.com/svg.latex?e_{obs}=x-\hat{x}" alt="latex_equ"><br><img src="https://latex.codecogs.com/svg.latex?\dot{x}=Ax+Bu" alt="latex_equ">, <img src="https://latex.codecogs.com/svg.latex?y=Cx" alt="latex_equ"><br><img src="https://latex.codecogs.com/svg.latex?\dot{\hat{x}}=A\hat{x}+Bu+K%28y-\hat{y}%29" alt="latex_equ">, <img src="https://latex.codecogs.com/svg.latex?\hat{y}=C\hat{x}" alt="latex_equ"><br>subtract<br><img src="https://latex.codecogs.com/svg.latex?\dot{e_{obs}}=%28A-KC%29e_{obs}%20\to%20e_{obs}%28t%29=e^{%28A-KC%29t}e_{obs}%280%29" alt="latex_equ"><br><img src="https://images-1256050009.cos.ap-beijing.myqcloud.com/15688965872265.jpg" alt="15688965872265"><br>Multiple the predicted position&#39;s p.d.f. and the measured position&#39;s, p.d.f., and form a new Gaussian Distribution.<a href="https://www.youtube.com/watch?v=ul3u2yLPwU0&amp;list=PLn8PRpmsu08pzi6EMiYnR-076Mh-q3tWr&amp;index=3">See more</a></p>
<h4 id="gating">Gating</h4>
<p><img src="https://latex.codecogs.com/svg.latex?x^i_k" alt="latex_equ"> means instance i&#39;s location in k time, subject to <img src="https://latex.codecogs.com/svg.latex?\hat{x}^i_k" alt="latex_equ">, <img src="https://latex.codecogs.com/svg.latex?\Sigma^i_k" alt="latex_equ"> Gaussian distribution. <img src="https://latex.codecogs.com/svg.latex?\hat{x}^i_k" alt="latex_equ">, <img src="https://latex.codecogs.com/svg.latex?\Sigma^i_k" alt="latex_equ"> can be estimated via Kalman Filter.<br>Use Mahalonobis Distance between observed location and predicted location to determine add to trajectory or not.<br><img src="https://latex.codecogs.com/svg.latex?d^2=%28\hat{x}^i_k-y^i_k%29^T%28\Sigma^i_k%29^{-1}%28\hat{x}^i_k-y^i_k%29\leq%20threshold" alt="latex_equ"><br>threshold determine range the gating area.</p>
<h2 id="plug-play-convolutional-regression-tracker-for-video-object-detection">Plug &amp; Play Convolutional Regression Tracker for Video Object Detection</h2>
<p>Detectorä¸­åŠ å…¥light-weight trackerï¼Œä½¿ç”¨detectoræå–çš„ç‰¹å¾</p>
<p><img src="Figures/image-20200304123522688.png" alt="image-20200304123522688"></p>
